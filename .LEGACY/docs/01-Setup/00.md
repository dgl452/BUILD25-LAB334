# 1. Overview

!!! quote "BY THE END OF THIS SECTION YOU SHOULD BE ABLE TO"

    - Explain the role of evaluations in the GenAIOps lifecycle
    - Understand the key tools and capabilities of the Azure AI Evaluations SDK
    - Understand how to apply these learnings to a relevant application scenario

!!! warning "IN-VENUE LEARNERS: Skip to the next section! This content was covered by instructors"

---

## Introduction

**Generative AI Operations** (GenAIOps), refers to the practice of managing, evaluating, and improving generative AI systems to ensure they produce trustworthy, reliable, and safe outputs throughout their lifecycle. The GenAIOps cycle can be viewed as 3 stages:

1. **Model Selection** - the first step is to _find the right model for your needs_
1. **App Development** - the next step is to _customize_ model behavior to suit requirements
1. **Operationalization** - the last step is to _monitor and optimize_ apps in production

![Genaiops](./../../img/1-Workshop/01-genaiops.png)

**Evaluations** are critical to this process, helping us gain user confidence and trust in the quality and safety of our applications at each step:

1. **Model Selection** - use a relevant dataset to evaluate models for fit
1. **App Development** - use built-in and custom evaluators to assess quality and safety
1. **Operationalization** - use tools to analyze results and optimize apps continuously

---

## Application Scenario

Understanding complex concepts is easier if we have an application scenario that we can use to _contextualize_ the discussion. Let's revisit this popular application scenario. **Contoso Outdoors** is a fictional enterprise retailer that sells outdoor hiking and camping equipment on their website. The figure shows a mockup of that experience.

![Storyboard](./../../img/1-Workshop/01-contoso-outdoors.png)

The popularity of the site has created a bottleneck for customer support. So they have asked you to build [**Contoso Chat**](https://aka.ms/aitour/contoso-chat) - a RAG-based retail copilot that can answer questions grounded in the product catalog and customer purchase history.

![Storyboard](./../../img/1-Workshop/01-contoso-chat.png)

!!! info "NOTE: This workshop does NOT build Contoso Chat."

    We are using the application scenario to frame the discussion on evaluation in a real-world context. However, each lab will use a toy dataset or app to teach the tools, metrics, and processes **for evaluation**. If you are interested in learning to build Contoso Chat as an application, check out our previous [AI Tour Workshops](https://aka.ms/aitour/contoso-chat/workshop) for details.

You are a new hire in that team - and you are tasked with the following:

1. **Model Selection** - find us the right model to use for the job
1. **Evaluation Dataset** - get us the right dataset to use for evaluations
1. **Evaluation Metrics** - identify evaluators we should use for quality & safety
1. **Custom Evaluators** - identify gaps in evaluation metrics that we should fill

_What do you do?_ Let's take you on the developer journey for evaluation.

---

## Developer Journey

The storyboard below visualizes the typical developer journey into evaluations. In this lab, we will cover many of these elements in the **Workshop** track - and provide bonus labs in the **Homework** track to help you continue the learning journey.

![Storyboard](./../../img/lab334.png)

### Setup: Create AI Project

Our application scenario requires model responses to be grounded in data. We will:

 - Setup an Azure AI project with an Azure AI Search resource connected
 - Upload our data and establish a search index for knowledge retrieval
 - We are now ready to take the next step in our GenAIOps workflow

!!! info "In Lab 334, we'll use a deployed version of Contoso Chat as our default Azure AI Foundry project"

### Step 1: Evaluate Base Models

Next, we need to select the right model to prototype our application. We can:

- Deploy one or more models from the [model catalog](https://ai.azure.com/explore/models) to choose from
- Use the default [benchmarks](https://ai.azure.com/explore/benchmarks) to compare and filter down the choices
- Use [manual evaluations](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/evaluate-prompts-playground) to assess responses against test prompts - without code!
- We can also use [code-first approaches](https://github.com/Azure-Samples/azureai-samples/tree/main/scenarios/evaluate/Supported_Evaluation_Targets/Evaluate_Base_Model_Endpoint) to compare models from code

!!! quote "At the end of this step, you should have selected your _chat_ model for app prototyping"

****

### Step 2: Create Test Dataset

Next, we need a way to _assess_ the quality and safety of our application prototype, as we develop it. Because these AI models are stochastic, we need to test them with a broad set of relevant "test prompts" to make sure they follow the guidelines we provide for responses.

You have three options:

1. _Bring your own data_ - the problem is most applications don't have historical query data.
1. _Create data manually_ - the problem is this requires non-trivial effort and has coverage gaps.
1. _Use a simulator_ - the Azure AI Evalution SDK creates _synthetic dataset_ from search indexes.
1. _Use adversarial simulators_ - create synthetic datasets that test the _safety_ of app responses.

!!! quote "At the end of this step, you should have an evaluation dataset you can use consistently"

---

### Step 3: Evaluate Metrics

Next, we need criteria that we can use to _rate_ the quality and safety of generated responses.

1. _Use built-in quality evaluators_ - use standard metrics (coherence, groundedness, relevance)
1. _Use built-in safety evaluators_ - use standard metrics to detect harmful, protected content etc.
1. _View results for insights_ - use portal-based dashboards to visualize and analyze results

!!! quote "At the end of this step, you should have an evaluation workflow that can run on each app iteration"

---


### Step 4: Customize Metrics

Does your application require custom assessments outside the scope of built-in evaluators?

- _Create custom evaluators_ - create custom prompt templates to "judge" model responses
- _Use custom evaluators_ - combine built-in and custom evaluators in your evaluation workflows

!!! quote "At the end of this step, you should have filled any assessment gaps from built-in metrics"

---

### Step 5: Operationize It

Think about how these evaluations can be _automated_ to streamline usage in production:

- Red teaming agents - proactively assess safety vulnerabilities in your application
- Content filters - customize the safety levels for your application, for various categories
- Monitoring - activate tracing and get application insights for optimization and debugging

!!! quote "At the end of this step, you should be able to iterate on apps in production"


## References

!!! info "Because of time-constraints, we will only cover a subset of these steps today. But we encourage you to explore more samples and tutorials using these references"

- [Azure AI Documentation](https://learn.microsoft.com/en-us/azure/ai-services/)
- [Azure Best Practices for AI](https://learn.microsoft.com/en-us/azure/architecture/example-scenario/ai/)
- [Azure AI Evaluation Tools](https://learn.microsoft.com/en-us/azure/machine-learning/how-to-evaluate-models)

These references provide additional context and detailed guidance on the tools and techniques covered in this lab.

