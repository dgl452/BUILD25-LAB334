# Lab 4: Evaluate Base Models

!!! quote "In this lab, you will how to evaluate base models for inital selection."

    By the end of this lab, you will know:

    - [X] How to compare multiple base models (during model selection)
    - [X] How to run evaluators across multiple models (for automated evaluation)

---

## Deploy Another Model

!!! quote "Our current Azure AI Project has a `gpt-4o-mini` model deployed. Let's add a second deployment (`gpt-35-turbo`) and evaluate both with the same set of evaluators and dataset, to see how they compare."

1. Return to the `ai.azure.com` tab
1. Click on the default Azure AI Project, select **Models + Endpoints**
1. Enter `gpt-35-turbo` in the search and select that model - click **Confirm**
1. In the pop-up dialog, click **Deploy** - Done!

_This takes less than a minute - but gives you a sense of how easy it is to add more models to compare against if you want to assess fit for your application needs_

---

## Instructions

!!! info "All labs are setup as Jupyter notebooks - just follow these instructions:"

1. Open the `labs/04-evaluate-models.ipynb` notebook in the VS Code editor.
1. Click **Select Kernel** in the top right corner of the notebook - pick default Python kernel.
1. Click **Clear All Outputs** in the top menu bar of the notebook - clears output from prior runs.
1. Click **Run All Cells** in the top menu bar of the notebook - let the run complete.

Now, review the notebook cell-by-cell to understand the steps. Answer questions or try alternative options when prompted, to build your understanding of the code.

---