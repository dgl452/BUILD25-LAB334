{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a57b6e53",
   "metadata": {},
   "source": [
    "# Lab 02: Explore Built-in Quality Evaluators\n",
    "\n",
    "By the end of this lab, you will know:\n",
    "\n",
    "1. What AI-Assisted evaluation workflows are, and how to run them.\n",
    "1. The built-in quality evaluators available in Azure AI Foundry\n",
    "1. How to run a quality evaluator with a test prompt (to understand usage)\n",
    "1. How to run a composite quality evaluator (with multiple evaluators)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "773df80e",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "---\n",
    "\n",
    "## 1. Generation Quality Evalution\n",
    "\n",
    "Generation quality metrics are used to assess the overall quality of the content produced by generative AI applications. \n",
    "\n",
    "All metrics or evaluators output a score and an explanation for the score (except for SimilarityEvaluator which currently outputs a score only). [Browse the documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in?tabs=warning#generation-quality-metrics) for details on how each metric works.\n",
    "\n",
    "In this lab, we are exploring the \"AI-assisted quality evaluator\" box in the workflow shown below.\n",
    "- The evaluator expects to receive a dataset that contains the responses from a chat model (ready for evaluation)\n",
    "- It will run the evaluations on each row in that dataset, and push the results to local file and/or portal\n",
    "- To understand each evaluator, we will show the cell **with a single test prompt** representing a row in this dataset\n",
    "\n",
    "![Quality](./00-assets/quality-evaluation-diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fbc829",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Understand Build-in Quality Evaluators\n",
    "\n",
    "The Azure AI Foundry plaform provides a comprehensive set of built-in quality evaluators that can be used to assess the performance of generative AI models. To keep on time, we'll cover **Generation** and **Custom** (code-based) examples in this notebook.  We encourage you to visit the [documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in?tabs=warning#generation-quality-metrics) and add more examples - for instance: \"Agents\", \"Custom (prompt-based)\" -- to your copy of the notebook as a homework exercise. \n",
    "\n",
    "\n",
    "![Quality Evaluators](./00-assets/quality-evaluators.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa5c2dc7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Explore Generation Evaluators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa611ecc",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Setup Required Dependencies\n",
    "\n",
    "# --------- Azure AI Project\n",
    "import os\n",
    "from pprint import pprint\n",
    "\n",
    "# The Azure AI Foundry connection string contains all the parameters we need\n",
    "connection_string = os.environ.get(\"AZURE_AI_CONNECTION_STRING\")\n",
    "region_id, subscription_id, resource_group_name, project_name = connection_string.split(\";\")\n",
    "\n",
    "# Use extracted values to create the azure_ai_project\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group_name\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "pprint(azure_ai_project)\n",
    "\n",
    "# ---------- Model Config\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"LAB_JUDGE_MODEL\"),\n",
    "}\n",
    "pprint(model_config)\n",
    "\n",
    "# ---------- Azure Credential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "credential=DefaultAzureCredential()\n",
    "pprint(credential)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea496bc9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.1 Retrieval Evaluator ðŸš¨\n",
    "\n",
    "1. See the [API](https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.retrievalevaluator?view=azure-python-preview)\n",
    "1. Read the [Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in?tabs=warning#ai-assisted-retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "343e5fb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import RetrievalEvaluator\n",
    "qEvaluator = RetrievalEvaluator(model_config)\n",
    "\n",
    "# Test 1: Provide a valid answer\n",
    "print(\"........ Evaluate with test response 1\")\n",
    "conversation = {\n",
    "    \"messages\": [\n",
    "        {\n",
    "            \"role\": \"user\", \n",
    "            \"content\": \"What is the value of 2 + 2?\"\n",
    "        },\n",
    "        {\n",
    "            \"role\": \"assistant\", \n",
    "            \"content\": \"2 + 2 = 4\",\n",
    "            \"context\": \"From 'math_doc.md': Information about additions: 1 + 2 = 3, 2 + 2 = 4\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "result = qEvaluator(conversation=conversation)\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5eda150a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "\n",
    "result = relevance_evaluator(\n",
    "    query=\"What is the capital of Japan?\",\n",
    "    response=\"The capital of Japan is Tokyo.\"\n",
    ")\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825840a1",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.2 Coherence Evaluator\n",
    "\n",
    "1. See the [API](https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.retrievalevaluator?view=azure-python-preview)\n",
    "1. Read the [Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in?tabs=warning#ai-assisted-retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da91363c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import CoherenceEvaluator\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "\n",
    "result = coherence_evaluator(\n",
    "    query=\"What is the capital of Japan?\",\n",
    "    response=\"The capital of Japan is Tokyo.\"\n",
    ")\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "978c7a6c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 3.3 Bleu/Gleu Evaluators\n",
    "\n",
    "1. See the [API](https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.retrievalevaluator?view=azure-python-preview)\n",
    "1. Read the [Documentation](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in?tabs=warning#ai-assisted-retrieval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f27387c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import GleuScoreEvaluator, BleuScoreEvaluator\n",
    "\n",
    "# NLP bleu score evaluator\n",
    "bleu_score_evaluator = BleuScoreEvaluator()\n",
    "result = bleu_score_evaluator(\n",
    "    response=\"Tokyo is the capital of Japan.\",\n",
    "    ground_truth=\"The capital of Japan is Tokyo.\"\n",
    ")\n",
    "pprint(result)\n",
    "\n",
    "# NLP gleu score evaluator\n",
    "gleu_score_evaluator = GleuScoreEvaluator()\n",
    "result = gleu_score_evaluator(\n",
    "    response=\"Tokyo is the capital of Japan.\",\n",
    "    ground_truth=\"The capital of Japan is Tokyo.\"\n",
    ")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d223752",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Explore Custom Evaluators"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9843c29",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 4.1 Code-Based Evaluator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5679169b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom evaluator as a function to calculate response length\n",
    "def response_length(response, **kwargs):\n",
    "    return len(response)\n",
    "\n",
    "# Custom class based evaluator to check for blocked words\n",
    "class BlocklistEvaluator:\n",
    "    def __init__(self, blocklist):\n",
    "        self._blocklist = blocklist\n",
    "\n",
    "    def __call__(self, *, answer: str, **kwargs):\n",
    "        contains_block_word = any(word in answer for word in self._blocklist)\n",
    "        return {\"score\": contains_block_word}\n",
    "\n",
    "blocklist_evaluator = BlocklistEvaluator(blocklist=[\"bad\", \"worst\", \"terrible\"])\n",
    "\n",
    "# Test custom evaluator 1\n",
    "result = response_length(\"The capital of Japan is Tokyo.\")\n",
    "print(result)\n",
    "\n",
    "# Test custom evaluator 2\n",
    "result = blocklist_evaluator(answer=\"The capital of Japan is Tokyo.\")\n",
    "print(result)\n",
    "\n",
    "# Test custom evaluator 3\n",
    "result = blocklist_evaluator(answer=\"This is a bad idea.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2107224d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# 5. Run Multiple Evaluators\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a70075c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    ContentSafetyEvaluator,\n",
    "    RelevanceEvaluator,\n",
    "    CoherenceEvaluator,\n",
    "    GroundednessEvaluator,\n",
    "    FluencyEvaluator,\n",
    "    SimilarityEvaluator,\n",
    ")\n",
    "\n",
    "# Create evaluators\n",
    "content_safety_evaluator = ContentSafetyEvaluator( azure_ai_project=azure_ai_project, credential=credential)\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "groundedness_evaluator = GroundednessEvaluator(model_config)\n",
    "fluency_evaluator = FluencyEvaluator(model_config)\n",
    "similarity_evaluator = SimilarityEvaluator(model_config)\n",
    "\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"00-data/02-data.jsonl\",\n",
    "    evaluators={\n",
    "        \"content_safety\": content_safety_evaluator,\n",
    "        \"coherence\": coherence_evaluator,\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"groundedness\": groundedness_evaluator,\n",
    "        \"fluency\": fluency_evaluator,\n",
    "        \"similarity\": similarity_evaluator,\n",
    "    },\n",
    "    evaluation_name=\"02-quality-evaluators\",\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"content_safety\": {\"column_mapping\": {\"query\": \"${data.query}\", \"response\": \"${data.response}\"}},\n",
    "        \"coherence\": {\"column_mapping\": {\"response\": \"${data.response}\", \"query\": \"${data.query}\"}},\n",
    "        \"groundedness\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.ground_truth}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"fluency\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"similarity\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"context\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "    },\n",
    "\n",
    "    # Specify the azure_ai_project to push results to portal\n",
    "    azure_ai_project = azure_ai_project,\n",
    "    \n",
    "    # Specify the output path to push results also to local file\n",
    "    output_path=\"./02-quality-evaluators.results.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "686630d4",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## ðŸŽ‰ | Congratulations!\n",
    "\n",
    "You have successfully completed the second lab in this module and got hands-on experience with a core subset of the the built-in quality evaluators. You also got a sense of how to create and run a custom evaluator."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71dceed0",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
