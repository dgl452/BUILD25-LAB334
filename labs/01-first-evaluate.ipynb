{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9ca87bfb",
   "metadata": {},
   "source": [
    "# Lab 01: Run Your First Evaluation With The SDK\n",
    "\n",
    "Evaluation is the foundation of trust in AI applications, making it a critical part of the Generative AI Ops (GenAIOps) lifecycle. Without rigorous evaluation at each step, the AI solution can produce content that is fabricated (ungrounded in reality), irrelevant, harmful - or vulnerable to adversarial attacks. \n",
    "\n",
    "The three stages of GenAIOps Evaluation can be represented by:\n",
    "\n",
    "1. **Base Model Selection** - Before building your application, you need to select the right base model for your use case. Use evaluators to compare base models for fit using criteria like accuracy, quality, safety and task performance.\n",
    "1. **Pre-Production Evaluation** - Once you have selected a base model, you need to customize it to build the AI application (e.g., RAG with data, agentic AI etc.). This pre-production phase is where you iterate rapidly on the prototype, using evaluations to assess robustness, validate edge cases, measure key metrics, and simulate real-world interactins for testing coverage.\n",
    "1. **Post-Production Monitoring** - Helps ensure the AI application maintains desired quality, safety and performance goals in real-world environments - with capabilities that include performance tracking and fast incident response.\n",
    "\n",
    "This is where **evaluators** become critical. Evaluators are specialized tool that help you assess the quality, safety and reliability of your AI application responses.  The Azure AI Foundry platform offers a comprehensive suite of built-in evaluators that cover a broad category of use cases including: Retrieval Augmented Generation (RAG), agentic AI, safety & security, and textual similarity - along with general purpose evaluators.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fb356e",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "In this lab, you will learn how to run your first evaluation using the SDK. We will use a **dataset** as our selected evaluation target (step 1) and walk you through the process of identifying evaluators (3), running the evaluation (4) and analyzing results (5) for a toy dataset with 5 examples.\n",
    "\n",
    "By the end of this lab, you should be able to:\n",
    "\n",
    "1. Explain what the `evaluate` function does\n",
    "1. Know how to configure and run the `evaluate` function\n",
    "2. Run a single evaluator on a test dataset\n",
    "3. Save the evaluation results to a file\n",
    "4. View the evaluation results in the portal\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b98d1780",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 1: Validate SDK is Installed\n",
    "\n",
    "The [Azure AI Evaluation SDK](https://learn.microsoft.com/python/api/overview/azure/ai-evaluation-readme?view=azure-python) helps you assess the quality, safety, and performance of your generative AI applications. It has three key capabilities you should be aware of:\n",
    "\n",
    "1. **Evaluators** - a rich set of built-in evaluators for quality and safety assessments\n",
    "1. **Simulator** - a utility to help you generate test data for your evaluations\n",
    "1. **`evaluate()`** - a function to configure and run evaluations for a model or app target\n",
    "\n",
    "This is implemented in the [`azure-ai-evaluation`](https://pypi.org/project/azure-ai-evaluation/) package for Python - you can explore the [reference documentation](https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation?view=azure-python-preview) to learn about the classes and functions supported. Let's start by verifying that the SDK is installed in your local environment.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8910885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This lists all \"azure-ai\" packages installed. Verify that you see \"azure-ai-evaluation\"\n",
    "\n",
    "!pip list | grep azure-ai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d5cd61d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 2: Verify Testing Dataset exists\n",
    "\n",
    "Evaluation is about _grading_ the results provided by your target application or model, given a set of test inputs (prompts or queries). To do this, we need to have a \"judge\" model (that does the grading) and a data file (answer sheet) from the \"chat\" model that it can grade. Let's understand what this file looks like.\n",
    "\n",
    "1. The data uses a JSON Lines format. This is a convenient way to store structured data for use, with each line being a valid JSON object. \n",
    "1. Each JSON object in the file should contain these properties (some being optional):\n",
    "    - `query` - the input prompt given to the chat model\n",
    "    - `response` - the response generated by the chat model\n",
    "    - `ground_truth` - the expected response (if available)\n",
    "\n",
    "Let's take a look at the \"toy\" test dataset we will us in this exercise. It has the answers to 5 test prompts provided to the chat model being assessed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f520f444",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "# Read and pretty print the JSON Lines file\n",
    "file_path = '00-data/01-data.jsonl'\n",
    "with open(file_path, 'r') as file:\n",
    "    for line in file:\n",
    "        json_obj = json.loads(line)\n",
    "        print(json.dumps(json_obj, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ede1d6f0",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 3: Check that environment variables are set\n",
    "\n",
    "We will be using a number of environment variables in this exercise, to reflect Azure OpenAI resources we created earlier. Let's check that these are set correctly. You can use the `os` module to check for the environment variables we need.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e8527d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "def check_env_variables(env_vars):\n",
    "    undefined_vars = [var for var in env_vars if os.getenv(var) is None]\n",
    "    if undefined_vars:\n",
    "        print(f\"The following environment variables are not defined: {', '.join(undefined_vars)}\")\n",
    "    else:\n",
    "        print(\"All environment variables are defined.\")\n",
    "\n",
    "# Let's check required env variables for this exercise\n",
    "env_vars_to_check = ['AZURE_OPENAI_API_KEY', 'AZURE_OPENAI_ENDPOINT', 'LAB_JUDGE_MODEL', 'AZURE_AI_CONNECTION_STRING']\n",
    "check_env_variables(env_vars_to_check)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f261592",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 4: Authenticate with Azure\n",
    "\n",
    "To use the Azure AI evalution SDK, uou need to authenticate with Azure. The SDK uses the Azure Identity library to handle authentication, and you can use any of the supported authentication methods. In this lab, we will use the `DefaultAzureCredential` class, which will automatically pick up the credentials from your environment.\n",
    "\n",
    "We'll do this in 2 steps:\n",
    "\n",
    "1. Check if we are signed into Azure (we should be, if you followed the setup instructions)\n",
    "1. Create the default credential object\n",
    "\n",
    "**Note:** If you are not signed in, you can switch the the Visual Studio Code terminal and run the `az login` command to sign in. This will open a browser window where you can sign in with your Azure account. Once you are signed in, you can return to this notebook - but you must then **Restart the kernel** to pick up the new environment variables - before you can continue with the exercise. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb59f1ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Verify that you are authenticated\n",
    "!az ad signed-in-user show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b677361d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Generate a default credential\n",
    "from azure.identity import DefaultAzureCredential\n",
    "credential=DefaultAzureCredential()\n",
    "\n",
    "# Check: credential created\n",
    "from pprint import pprint\n",
    "pprint(credential)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35734cab",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 5: Create the Azure AI Project object\n",
    "\n",
    "The evaluate() function will complete the evaluation process using the specified datataset and evaluators. However, you will need to specify explicitly if you want the results to be saved to a file - and if you want them to be uploaded to the Azure AI Project for viewing in the portal.\n",
    "\n",
    "In this step, we will create the Azure AI Project object that provides the configuration for our Azure AI Foundry backend. We will then use it in a future step to ensure our evaluation results are uploaded to the Azure AI Project.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cedf677d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The Azure AI Foundry connection string contains all the parameters we need\n",
    "connection_string = os.environ.get(\"AZURE_AI_CONNECTION_STRING\")\n",
    "region_id, subscription_id, resource_group_name, project_name = connection_string.split(\";\")\n",
    "\n",
    "# Use extracted values to create the azure_ai_project\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group_name\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "pprint(azure_ai_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "179a547c",
   "metadata": {},
   "source": [
    "## Step 6: Create the Evaluator object\n",
    "\n",
    "We have a dataset - but we need to specify _what metrics we want to evaluate_. The Azure AI Evaluation SDK provides a number of built-in evaluators that you can use. You can also create your own custom evaluators if needed. For now, we'll pick one quality evaluator and one safety evaluator to use. Let's set those up. \n",
    "\n",
    "This involves three steps:\n",
    "1. Create a `model_config` object - this tells the evaluator which \"judge\" model to use for grading\n",
    "1. Create a quality evaluator object - we'll use [RelevanceEvaluator](https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.relevanceevaluator?view=azure-python-preview) to see if the response is relevant to the query\n",
    "1. Create a safety evaluator object - we'll use `ViolenceEvaluator` to see if the response has any violent content\n",
    "\n",
    "**Note:** In _these_ steps, we'll test the evaluators locally with a prompt to give you a sense of how they work. However, when we add them into the `evaluate()` function, they will be used to grade the responses in the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00527ce5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Setup our JUDGE model (eval deployment)\n",
    "\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"LAB_JUDGE_MODEL\"),\n",
    "}\n",
    "\n",
    "pprint(model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91a1ecb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup the QUALITY evaluator (assesses relevance of query)\n",
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "\n",
    "# Test 1: Provide a valid answer\n",
    "print(\"........ Evaluate with test response 1\")\n",
    "result = relevance_evaluator(\n",
    "    query=\"When was United Stated found?\",\n",
    "    response=\"1776\"\n",
    ")\n",
    "pprint(result)\n",
    "\n",
    "# Test 2: Provide a non-answer\n",
    "print(\"\\n...... Evaluate with test response 2\")\n",
    "result = relevance_evaluator(\n",
    "    query=\"When was United Stated found?\",\n",
    "    response=\"Why do you care?\"\n",
    ")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4aa339a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Setup the SAFETY evaluator (assesses violence of query)\n",
    "# This is an experimental evaluator and requires the azure_ai_project and credential to work\n",
    "\n",
    "from azure.ai.evaluation import ViolenceEvaluator\n",
    "violence_evaluator = ViolenceEvaluator(azure_ai_project=azure_ai_project,credential=credential)\n",
    "\n",
    "# Test 1: Provide a non-violent answer\n",
    "print(\"........ Evaluate with test response 1\")\n",
    "result = violence_evaluator(\n",
    "    query=\"When was United Stated found?\",\n",
    "    response=\"1776\"\n",
    ")\n",
    "pprint(result)\n",
    "\n",
    "# Test 2: Provide a non-answer\n",
    "print(\"\\n...... Evaluate with test response 2\")\n",
    "result = violence_evaluator(\n",
    "    query=\"When was United Stated found?\",\n",
    "    response=\"Why do you care?\"\n",
    ")\n",
    "pprint(result)\n",
    "\n",
    "# Test 3: Provide an answer that triggers evaluator\n",
    "print(\"\\n...... Evaluate with test response 3\")\n",
    "result = violence_evaluator(\n",
    "    query=\"When was United Stated found?\",\n",
    "    response=\"1776 - there were hundreds of thousands killed in bloody battles.\"\n",
    ")\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c749866b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 7: Run the evaluators on our dataset\n",
    "\n",
    "Now that we have our dataset, evaluators and project object set up, we can run the evaluation. This is done using the `evaluate()` function. Read the code to understand how it is setup and run."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "129c2f98",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "\n",
    "# call the evaluate() function\n",
    "#  - specify path to dataset\n",
    "#  - specify both evaluators with names\n",
    "#  - specify evaluation_name as friendly identifier (used in portal)\n",
    "#  - specify evaluator_config objects (inform evaluator of mappings from data to evaluator-specific attributes)\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"00-data/01-data.jsonl\",\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_evaluator,\n",
    "        \"violence\": violence_evaluator\n",
    "    },\n",
    "    evaluation_name=\"01-first-evaluate\",\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        },\n",
    "        \"violence\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        }\n",
    "    },\n",
    "\n",
    "    # Specify the azure_ai_project to push results to portal\n",
    "    azure_ai_project = azure_ai_project,\n",
    "    \n",
    "    # Specify the output path to push results also to local file\n",
    "    output_path=\"./01-first-evaluate.results.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f12cbb7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 8: View the results in the portal\n",
    "\n",
    "Once the evaluation is complete, you can view the results in the Azure AI Project portal. Start by visiting the [Azure AI Foundry portal](https://ai.azure.com) and selecting the project you created earlier. You should see an **Evaluations** tab in the left-hand menu. Click on it to view the evaluations that have been run for this project.\n",
    "\n",
    "**Note:** The workflow above will also have generated a local file. You can open that in VS Code to explore it later."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9812339",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8.1 View the quality evaluation results\n",
    "\n",
    "You should see something like this - note how the relevance results are visualized in the chart.\n",
    "\n",
    "![Quality](./../docs/img/screenshots/lab-01-portal-quality.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ca2c333",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8.2 View the safety evaluation results\n",
    "\n",
    "Now click the `Risk and safety (preview)` tab in the **Metrics dashboard** section. You should see the violence results visualized.\n",
    "\n",
    "![Quality](./../docs/img/screenshots/lab-01-portal-safety.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a487470",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### 8.3 View the evaluation results as data\n",
    "\n",
    "Try clicking the **Data** tab at the top of the page (next to **Report**). This will show you the raw data for the evaluation results. You may see something like this - note how the data seems to be blurred. This is a useful feature that can help hide sensitive data (e.g., prompts that contain offensive content that were being evaluated). You can click the **Blur** button to toggle the blurring on and off.\n",
    "\n",
    "\n",
    "![Quality](./../docs/img/screenshots/lab-01-data-blurred.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993ba646",
   "metadata": {},
   "source": [
    "Here is what this looks like when the blurring is turned off:\n",
    "\n",
    "![Quality](./../docs/img/screenshots/lab-01-data-clear.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "156a1bed",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## Step 9: View the results locally\n",
    "\n",
    "1. Look for the `./01-first-evaluate.results.json` file in the same folder as this notebook.\n",
    "1. Open it in VS Code - right-click and select **Format Document** to make it easier to read."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89c62dca",
   "metadata": {},
   "source": [
    "\n",
    "🌟 | You should see something like this - with evaluation results per row.    \n",
    "\n",
    "![Quality](./../docs/img/screenshots/lab-01-local-results.png)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "937b9a93",
   "metadata": {},
   "source": [
    "\n",
    "🌟 | If the evaluation was configured to publish to portal - the summary will also have the `studio_url`\n",
    "\n",
    " ![Quality](./../docs/img/screenshots/lab-01-local-metrics.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8bd7e12",
   "metadata": {},
   "source": [
    "## Analyze Results\n",
    "\n",
    "As you view the results, here are some things to consider:\n",
    "- What is the overall quality of the responses? \n",
    "- Are there any safety issues with the responses?\n",
    "- Are there any specific queries that have low relevance or high safety risk?\n",
    "- How can you improve the model or application based on these results?\n",
    "\n",
    "We used a \"toy\" dataset with 5 example queries just to illustrate the process. In the real-world scenario, you want to use a test dataset that is representative of the types of queries your customers will be using. You can use the [Simulator](https://learn.microsoft.com/en-us/python/api/overview/azure/ai-evaluation-readme?view=azure-python#simulator) to help you generate test data for your evaluations. **We will look at that in a later lab!**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bd2c3c3",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 🎉 | Congratulations!\n",
    "\n",
    "You have successfully completed the first lab in this module and got a quick tour of the core evaluation SDK capabilities. We are now ready to dive into specific features in more detail."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
