{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Base Model Selection](https://learn.microsoft.com/azure/ai-foundry/concepts/evaluation-approach-gen-ai#base-model-selection)\n",
    "\n",
    "The first stage of the AI lifecycle involves selecting an appropriate base model. Generative AI models vary widely in terms of capabilities, strengths, and limitations, so it's essential to identify which model best suits your specific use case. During base model evaluation, you \"shop around\" to compare different models by testing their outputs against a set of criteria relevant to your application.\n",
    "\n",
    "You have three options to evaluate models:\n",
    "\n",
    "1. [Use Azure AI Foundry Benchmarks](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/model-benchmarks) to compare models on their intrinsic capabilities.\n",
    "1. [Use Manual Evaluations in the Portal](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/evaluate-prompts-playground) to run prompts on models and rate them.\n",
    "1. [Evaluate Multiple Models using the SDK](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/evaluate-prompts-playground) code-first\n",
    "\n",
    "**In this notebook, we explore a simplified version of option 3**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Objective\n",
    "\n",
    "You are beginning your AI application development journey - and you have two (or more) model options available to you. How do you pick the right one for your needs? In this tutorial we look at how you can evaluate _the same set of prompts_ against multiple model endpoints deployed in your Azure AI project.\n",
    "\n",
    "This guide uses Python Class as an application target which is passed to Evaluate API provided by PromptFlow SDK to evaluate results generated by LLM models against provided prompts. \n",
    "\n",
    "This tutorial uses the following Azure AI services:\n",
    "\n",
    "- [azure-ai-evaluation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk)\n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend 30 minutes running this sample. \n",
    "\n",
    "## About this example\n",
    "\n",
    "This example demonstrates evaluating model endpoints responses against provided prompts using azure-ai-evaluation\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validate Required Environment Variables are set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "assert os.environ.get(\"AZURE_OPENAI_ENDPOINT\") is not None, \"Please set the AZURE_OPENAI_ENDPOINT environment variable.\"\n",
    "assert os.environ.get(\"AZURE_OPENAI_API_VERSION\") is not None, \"Please set the AZURE_OPENAI_API_VERSION environment variable.\"\n",
    "assert os.environ.get(\"AZURE_OPENAI_API_KEY\") is not None, \"Please set the AZURE_OPENAI_API_KEY environment variable.\"\n",
    "assert os.environ.get(\"AZURE_AI_CONNECTION_STRING\") is not None, \"Please set the AZURE_AI_CONNECTION_STRING environment variable.\"\n",
    "assert os.environ.get(\"LAB_JUDGE_MODEL\") is not None, \"Please set the LAB_JUDGE_MODEL environment variable.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Evaluation\n",
    "\n",
    "We will use Evaluate API provided by Azure AI Evaluation SDK. In the notebook, we will use different models and evaluate them. Azure AI Foundry will be used to visualize and compare results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Getting Azure AI Foundry Project details"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Project Connection String\n",
    "connection_string = os.environ.get(\"AZURE_AI_CONNECTION_STRING\")\n",
    "\n",
    "# Extract details\n",
    "region_id, subscription_id, resource_group_name, project_name = connection_string.split(\";\")\n",
    "\n",
    "# Populate it\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group_name\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "print(azure_ai_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Following code reads Json file \"data.jsonl\" which queries that will passed to each model for evaluation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_json(\"00-data/05-data.jsonl\", lines=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "To use Relevance and Cohenrence Evaluator, we will Azure Open AI model details as a Judge that can be passed as model config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "\n",
    "# Note: We are evaluating 2 models above - and we need a \"LLM Judge\" to evaluate them\n",
    "#       Here we specify the judge model\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.environ.get(\"LAB_JUDGE_MODEL\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the evaluation\n",
    "\n",
    "The Following code runs Evaluate API and Relevance, Coherence (LLM as Judge), Bleu, Rogue (NLP) and Violence (Content Safety) Evaluator to evaluate results from different models.\n",
    "\n",
    "The following are the few parameters required by Evaluate API. \n",
    "\n",
    "+   Data file (Prompts): It represents data file 'data.jsonl' in JSON format. Each line contains question, context and ground truth for evaluators.     \n",
    "\n",
    "+   Application Target: It is name of python class which calls the model.  \n",
    "\n",
    "+   Model Name: It is an identifier of model so that custom code in the App Target class can identify the model type and call respective LLM model using endpoint URL and auth key.  \n",
    "\n",
    "+   Evaluators: List of evaluators is provided, to evaluate given prompts (questions) as input and output (answers) from LLM models. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize the evaluators  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib\n",
    "\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    RelevanceEvaluator, CoherenceEvaluator, BleuScoreEvaluator, RougeScoreEvaluator, RougeType, ViolenceEvaluator,\n",
    ")\n",
    "from azure.identity import DefaultAzureCredential\n",
    "\n",
    "# LLM as judge evaluator\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "coherence_evaluator = CoherenceEvaluator(model_config)\n",
    "\n",
    "# NLP evaluators\n",
    "blue_score_evaluator = BleuScoreEvaluator()\n",
    "rouge_score_evaluator = RougeScoreEvaluator(rouge_type=RougeType.ROUGE_1)\n",
    "\n",
    "# Violence evaluator\n",
    "violence_evaluator = ViolenceEvaluator(azure_ai_project=azure_ai_project, credential=DefaultAzureCredential())\n",
    "\n",
    "# Define the models to evaluate\n",
    "\n",
    "\n",
    "path = str(pathlib.Path(pathlib.Path.cwd())) + \"/00-data/04-data.jsonl\"\n",
    "print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function that uses `evaluate` API from `azure-ai-evaluation` package to Evaluate a Base Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from model_endpoints import BaseModel\n",
    "\n",
    "def evaluate_model(model):\n",
    "    results = evaluate(\n",
    "        evaluation_name=f\"Base Model Evaluation {model}\",\n",
    "        data=path,\n",
    "        target=BaseModel(model),\n",
    "        evaluators={\n",
    "            \"relevance\": relevance_evaluator,\n",
    "            \"coherence\": coherence_evaluator,\n",
    "            \"blue_score\": blue_score_evaluator,\n",
    "            \"rouge_score\": rouge_score_evaluator,\n",
    "            \"violence_score\": violence_evaluator,\n",
    "        },\n",
    "        azure_ai_project=azure_ai_project,\n",
    "    )\n",
    "    return results\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate GPT 35 Turbo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_35_turbo_results = evaluate_model(\"gpt-35-turbo\")\n",
    "pd.DataFrame(gpt_35_turbo_results[\"rows\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### AI Foundry URL to view results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f\"AI Foundry Studio URL: {gpt_35_turbo_results['studio_url']}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluate GPT 4o Mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gpt_4o_mini_results = evaluate_model(\"gpt-4o-mini\")\n",
    "pd.DataFrame(gpt_4o_mini_results[\"rows\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## View Results In Portal\n",
    "\n",
    "You can now visit the Azure AI Foundry portal.\n",
    "\n",
    "1. Click the Evaluations tab to see the results from the 2 runs. \n",
    "1. You should see something like this. **Select the two runs as shown, so we can do a visual comparison of the metrics**\n",
    "\n",
    "![base](./../docs/img/screenshots/lab-04-base-model-evals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compare Metrics In Dashboard\n",
    "\n",
    "1. At the top right of that panel, you will see a `Switch to Dashboard View` option. Click it!\n",
    "1. In the resulting page, **collapse** the `Comparison` tab - this will `Charts` into view\n",
    "1. You should see something like this - use the color cues to visually compare metrics\n",
    "\n",
    "**For example, the chart shows that the relevance scores (chart 1) are marginally better with gpt-4o-mini (blue) for this small test dataset**\n",
    "\n",
    "![](./../docs/img/screenshots/lab-04-base-models-dashboard.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}