{"rows": [{"inputs.query": "When was United Stated found ?", "inputs.ground_truth": "1776", "inputs.response": "1600", "outputs.relevance.relevance": 2, "outputs.relevance.gpt_relevance": 2, "outputs.relevance.relevance_reason": "The RESPONSE attempts to answer the QUERY but provides incorrect information, making it a clear example of a [Relevance: 2] rating.", "outputs.relevance.relevance_result": "fail", "outputs.relevance.relevance_threshold": 3, "line_number": 0}, {"inputs.query": "What is the capital of France?", "inputs.ground_truth": "Paris", "inputs.response": "Paris", "outputs.relevance.relevance": 4, "outputs.relevance.gpt_relevance": 4, "outputs.relevance.relevance_reason": "The RESPONSE fully and accurately addresses the QUERY without any omissions or irrelevant information, making it a complete response.", "outputs.relevance.relevance_result": "pass", "outputs.relevance.relevance_threshold": 3, "line_number": 1}, {"inputs.query": "Which tent is the most waterproof?", "inputs.ground_truth": "The Alpine Explorer Tent has the highest rainfly waterproof rating at 3000m", "inputs.response": "Can you clarify what tents you are talking about?", "outputs.relevance.relevance": 1, "outputs.relevance.gpt_relevance": 1, "outputs.relevance.relevance_reason": "The RESPONSE does not address the QUERY and provides no relevant information about waterproof tents, making it unrelated to the question.", "outputs.relevance.relevance_result": "fail", "outputs.relevance.relevance_threshold": 3, "line_number": 2}, {"inputs.query": "Which camping table holds the most weight?", "inputs.ground_truth": "The Adventure Dining Table has a higher weight capacity than all of the other camping tables mentioned", "inputs.response": "Adventure Dining Table", "outputs.relevance.relevance": 3, "outputs.relevance.gpt_relevance": 3, "outputs.relevance.relevance_reason": "The response is incomplete because it does not provide sufficient details to confirm or explain why the \"Adventure Dining Table\" holds the most weight.", "outputs.relevance.relevance_result": "pass", "outputs.relevance.relevance_threshold": 3, "line_number": 3}, {"inputs.query": "What is the weight of the Adventure Dining Table?", "inputs.ground_truth": "The Adventure Dining Table weighs 15 lbs", "inputs.response": "It's a lot I can tell you", "outputs.relevance.relevance": 1, "outputs.relevance.gpt_relevance": 1, "outputs.relevance.relevance_reason": "The RESPONSE does not provide any relevant or specific information about the weight of the Adventure Dining Table, making it completely off-topic and irrelevant to the QUERY.", "outputs.relevance.relevance_result": "fail", "outputs.relevance.relevance_threshold": 3, "line_number": 4}], "metrics": {"relevance.relevance": 2.2, "relevance.gpt_relevance": 2.2, "relevance.relevance_threshold": 3.0, "relevance.binary_aggregate": 0.4}, "studio_url": "https://ai.azure.com/build/evaluation/1f8a889f-8d6c-43b5-a7d1-b12df4ebd68c?wsid=/subscriptions/535d2669-627f-484f-aaad-68518bbe5cc5/resourceGroups/rg-AITOUR/providers/Microsoft.MachineLearningServices/workspaces/nnarasimhan-5110"}