{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "fb0ec5a1",
   "metadata": {},
   "source": [
    "# Verify Code-first Setup\n",
    "\n",
    "The [Azure AI Evaluation client library](https://pypi.org/project/azure-ai-evaluation/) helps you assess the performance of your generative AI applications.\n",
    "\n",
    "1. **Evaluate data results** from your generative AI applications\n",
    "1. **Evaluate app targets** directly to assess live responses\n",
    "1. **Evaluate using AI-assessted metrics** for quality and safety\n",
    "\n",
    "It does this with three core features:\n",
    "\n",
    "1. **Simulators** - to help you generate synthetic datasets for evaluation\n",
    "1. **Built-in Evaluators** - to help you evaluate the performance of your generative AI applications\n",
    "1. **evaluate()** - API to help you run evaluations in bulk model, using multiple evaluators and datasets\n",
    "\n",
    "_In this notebook, we will verify the code-first setup of the Azure AI Evaluation client library._"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c886a132",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Verify Installed Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3349c345",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that you have the azure-ai libraries installed\n",
    "!pip list | grep azure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6650a773",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that you have the laatest openai libraries installed\n",
    "!pip list | grep openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ba33eb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that you are logged into Azure, for generating keyless auth credentials\n",
    "!az ad signed-in-user show"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94149dbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import os\n",
    "import json\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c31afb9f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a default credential\n",
    "# You must be logged into Azure first (az login --use-device-code)\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "credential=DefaultAzureCredential()\n",
    "pprint(credential)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "328d148b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the azure-ai-project object\n",
    "\n",
    "# Project Connection String\n",
    "connection_string = os.environ.get(\"AZURE_AI_CONNECTION_STRING\")\n",
    "\n",
    "# Extract details\n",
    "region_id, subscription_id, resource_group_name, project_name = connection_string.split(\";\")\n",
    "\n",
    "# Populate it\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group_name\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "pprint(azure_ai_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8c2bbdc",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Try an NLP Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cceea168",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import BleuScoreEvaluator\n",
    "\n",
    "# NLP bleu score evaluator\n",
    "bleu_score_evaluator = BleuScoreEvaluator()\n",
    "result = bleu_score_evaluator(\n",
    "    response=\"Tokyo is the capital of Japan.\",\n",
    "    ground_truth=\"The capital of Japan is Tokyo.\"\n",
    ")\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56e3acda",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Try an AI-Assisted Quality Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bea93ec9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "\n",
    "# AI assisted quality evaluator\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_EVAL_DEPLOYMENT\"),\n",
    "}\n",
    "\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "result = relevance_evaluator(\n",
    "    query=\"What is the capital of Japan?\",\n",
    "    response=\"The capital of Japan is Tokyo.\"\n",
    ")\n",
    "\n",
    "pprint(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c781b91c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Try an AI-Assisted Safety Evaluator\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a155e7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import ViolenceEvaluator\n",
    "\n",
    "# AI assisted safety evaluator\n",
    "violence_evaluator = ViolenceEvaluator(azure_ai_project=azure_ai_project,credential=credential)\n",
    "result = violence_evaluator(\n",
    "    query=\"What is the capital of France?\",\n",
    "    response=\"Paris.\"\n",
    ")\n",
    "pprint(result)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e150a398",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 5. Try a Custom Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89dd8408",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom evaluator as a function to calculate response length\n",
    "def response_length(response, **kwargs):\n",
    "    return len(response)\n",
    "\n",
    "# Custom class based evaluator to check for blocked words\n",
    "class BlocklistEvaluator:\n",
    "    def __init__(self, blocklist):\n",
    "        self._blocklist = blocklist\n",
    "\n",
    "    def __call__(self, *, answer: str, **kwargs):\n",
    "        contains_block_word = any(word in answer for word in self._blocklist)\n",
    "        return {\"score\": contains_block_word}\n",
    "\n",
    "blocklist_evaluator = BlocklistEvaluator(blocklist=[\"bad\", \"worst\", \"terrible\"])\n",
    "\n",
    "# Test custom evaluator 1\n",
    "result = response_length(\"The capital of Japan is Tokyo.\")\n",
    "print(result)\n",
    "\n",
    "# Test custom evaluator 2\n",
    "result = blocklist_evaluator(answer=\"The capital of Japan is Tokyo.\")\n",
    "print(result)\n",
    "\n",
    "# Test custom evaluator 3\n",
    "result = blocklist_evaluator(answer=\"This is a bad idea.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6424311c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 6. Try a Composite Evaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20e5ae2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azure.ai.evaluation import evaluate, QAEvaluator\n",
    "\n",
    "qa_evaluator = QAEvaluator(model_config=model_config)\n",
    "\n",
    "eval_output = evaluate(\n",
    "    data=str(\"data.jsonl\"),\n",
    "    evaluators={\"QAEvaluator\": qa_evaluator},\n",
    "    evaluation_name=\"06-using-composite evaluator\",\n",
    "    evaluator_config={\n",
    "        \"QAEvaluator\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"context\": \"${data.ground_truth}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    \n",
    "    # Optionally provide your AI Foundry project information to track your evaluation results in your Azure AI Foundry project\n",
    "    azure_ai_project = azure_ai_project,\n",
    "    \n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and AI Foundry URL\n",
    "    output_path=\"./data_composite_results.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34fd0ece",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 7. Try the Simulator\n",
    "\n",
    "Simulators allow users to generate synthentic data using their application. Simulator expects the user to have a callback method that invokes their AI application. The intergration between your AI application and the simulator happens at the callback method. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7571332",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation.simulator import Simulator\n",
    "\n",
    "simulator = Simulator(model_config=model_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54dc15d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is my \"application\" for generting simulated responses from input text\n",
    "\n",
    "from typing import List, Dict, Any, Optional\n",
    "from openai import AzureOpenAI\n",
    "from azure.identity import DefaultAzureCredential, get_bearer_token_provider\n",
    "\n",
    "# This is the application chat_completion endpoin\n",
    "def call_to_your_ai_application(query: str) -> str:\n",
    "    # logic to call your application\n",
    "    # use a try except block to catch any errors\n",
    "    token_provider = get_bearer_token_provider(\n",
    "        DefaultAzureCredential(), \n",
    "        \"https://cognitiveservices.azure.com/.default\"\n",
    "    )\n",
    "\n",
    "    deployment = os.environ.get(\"AZURE_OPENAI_EVAL_DEPLOYMENT\")\n",
    "    endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "    client = AzureOpenAI(\n",
    "        azure_endpoint=endpoint,\n",
    "        api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    "        azure_ad_token_provider=token_provider,\n",
    "    )\n",
    "    completion = client.chat.completions.create(\n",
    "        model=deployment,\n",
    "        messages=[\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": query,\n",
    "            }\n",
    "        ],\n",
    "        max_tokens=800,\n",
    "        temperature=0.7,\n",
    "        top_p=0.95,\n",
    "        frequency_penalty=0,\n",
    "        presence_penalty=0,\n",
    "        stop=None,\n",
    "        stream=False,\n",
    "    )\n",
    "    message = completion.to_dict()[\"choices\"][0][\"message\"]\n",
    "    # change this to return the response from your application\n",
    "    return message[\"content\"]\n",
    "\n",
    "# This is the callback function that is called by the Simulator\n",
    "# It takes the messages from the simulator and calls your application\n",
    "async def callback(\n",
    "    messages: List[Dict],\n",
    "    stream: bool = False,\n",
    "    session_state: Any = None,  # noqa: ANN401\n",
    "    context: Optional[Dict[str, Any]] = None,\n",
    ") -> dict:\n",
    "    messages_list = messages[\"messages\"]\n",
    "    # get last message\n",
    "    latest_message = messages_list[-1]\n",
    "    query = latest_message[\"content\"]\n",
    "    context = None\n",
    "    # call your endpoint or ai application here\n",
    "    response = call_to_your_ai_application(query)\n",
    "    # we are formatting the response to follow the openAI chat protocol format\n",
    "    formatted_response = {\n",
    "        \"content\": response,\n",
    "        \"role\": \"assistant\",\n",
    "        \"context\": {\n",
    "            \"citations\": None,\n",
    "        },\n",
    "    }\n",
    "    messages[\"messages\"].append(formatted_response)\n",
    "    return {\"messages\": messages[\"messages\"], \"stream\": stream, \"session_state\": session_state, \"context\": context}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f94fdcaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# In this example we use a wikipedia article as raw text to generate Query Response pairs. \n",
    "import wikipedia\n",
    "\n",
    "wiki_search_term = \"Leonardo da vinci\"\n",
    "wiki_title = wikipedia.search(wiki_search_term)[0]\n",
    "wiki_page = wikipedia.page(wiki_title)\n",
    "text = wiki_page.summary[:5000]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "153a7e0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Call the Simulator\n",
    "outputs = await simulator(\n",
    "    target=callback,\n",
    "    text=text,\n",
    "    num_queries=4,\n",
    "    max_conversation_turns=3,\n",
    "    tasks=[\n",
    "        f\"I am a student and I want to learn more about {wiki_search_term}\",\n",
    "        f\"I am a teacher and I want to teach my students about {wiki_search_term}\",\n",
    "        f\"I am a researcher and I want to do a detailed research on {wiki_search_term}\",\n",
    "        f\"I am a statistician and I want to do a detailed table of factual data concerning {wiki_search_term}\",\n",
    "    ],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "391701a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the output to a file\n",
    "from pathlib import Path\n",
    "\n",
    "output_file = Path(\"data_simulation.json\")\n",
    "with output_file.open(\"a\") as f:\n",
    "    json.dump(outputs, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2769f4a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now you can run evaluations on the \"simulated\" dataset\n",
    "# Here we will try to run the following evaluators:\n",
    "#   GroundednessEvaluator, \n",
    "#   RelevanceEvaluator, \n",
    "#   CoherenceEvaluator, \n",
    "#   FluencyEvaluator, \n",
    "#   SimilarityEvaluator, \n",
    "#   F1ScoreEvaluator \n",
    "#\n",
    "# From the documentation we know that running those evaluators needs \n",
    "# the following data:\n",
    "# { query, response, context, ground_truth }\n",
    "#\n",
    "# For simplicity's sake, we can use our source document text as both \n",
    "# context and ground_truth. This step only evaluates the first user message \n",
    "# and first response from your AI Application for each of the simulated \n",
    "# conversations. **LET'S CREATE THE JSONL**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e5e7e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write the data to a variable in json format\n",
    "data_simulation_jsonl = \"\"\n",
    "for output in outputs:\n",
    "    query = None\n",
    "    response = None\n",
    "    context = text\n",
    "    ground_truth = text\n",
    "    for message in output[\"messages\"]:\n",
    "        if message[\"role\"] == \"user\":\n",
    "            query = message[\"content\"]\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            response = message[\"content\"]\n",
    "    if query and response:\n",
    "        data_simulation_jsonl += (\n",
    "            json.dumps(\n",
    "                {\n",
    "                    \"query\": query,\n",
    "                    \"response\": response,\n",
    "                    \"context\": context,\n",
    "                    \"ground_truth\": ground_truth,\n",
    "                }\n",
    "            )\n",
    "            + \"\\n\"\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4e4a095",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Store that to a JSONL file format for evaluations\n",
    "data_simulation_jsonl_file = Path(\"data_simulation.jsonl\")\n",
    "with data_simulation_jsonl_file.open(\"w\") as f:\n",
    "    f.write(data_simulation_jsonl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdaed79a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now run the evaluation using this JSONL with a QAEvaluator \n",
    "# This is a compositve evaluator that conveniently runs \n",
    "# GroundednessEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator, F1ScoreEvaluator\n",
    "\n",
    "# Optionally set the azure_ai_project to upload the evaluation results to \n",
    "# Azure AI Foundry\n",
    "\n",
    "from azure.ai.evaluation import evaluate, QAEvaluator\n",
    "\n",
    "qa_evaluator = QAEvaluator(model_config=model_config)\n",
    "\n",
    "eval_output = evaluate(\n",
    "    data=str(data_simulation_jsonl_file),\n",
    "    evaluators={\"QAEvaluator\": qa_evaluator},\n",
    "    evaluation_name=\"07-using-simulator-data\",\n",
    "    evaluator_config={\n",
    "        \"QAEvaluator\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"response\": \"${data.response}\",\n",
    "                \"context\": \"${data.context}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "            }\n",
    "        }\n",
    "    },\n",
    "    azure_ai_project=azure_ai_project,  # optional to store the evaluation results in Azure AI Studio\n",
    "    output_path=\"./data_simulation_eval_results.json\",  # optional to store the evaluation results in a file\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6093632",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 8. Try the `evaluate()` API"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8453ffcd",
   "metadata": {},
   "source": [
    "### 6.1 Generate the data in JSONL format \n",
    "\n",
    "I used this prompt with Copilot\n",
    "\n",
    "```bash title=\"\" linenums=\"0\"\n",
    "create a JSONL file called data.jsonl in the notebooks folder. \n",
    "Make sure it has 5 lines - each has {query, truth, response} propertoes - where the query can be related to camoing or hiking equipment\n",
    "````"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f26a0b74",
   "metadata": {},
   "source": [
    "### 6.2 Run the evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2432e50",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "\n",
    "# provide your data here\n",
    "data=\"data.jsonl\",\n",
    "\n",
    "# configure your quality evaluators here\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"data.jsonl\", # provide your data here\n",
    "    evaluators={\n",
    "        #\"blocklist\": blocklist_evaluator,\n",
    "        \"relevance\": relevance_evaluator\n",
    "    },\n",
    "    evaluation_name=\"08-using-evaluate-api\",\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        }\n",
    "    },\n",
    "    # Optionally provide your AI Foundry project information to track your evaluation results in your Azure AI Foundry project\n",
    "    azure_ai_project = azure_ai_project,\n",
    "    \n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and AI Foundry URL\n",
    "    output_path=\"./data_evaluation_results.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6ff95da",
   "metadata": {},
   "source": [
    "### 6.3 Run evaluation on app"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c0db849",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that you have the latest BeautifulSoup libraries installed\n",
    "!pip list | grep bs4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f7f380a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check that you have the latest jinja2 libraries installed\n",
    "!pip list | grep jinja2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e530cc",
   "metadata": {},
   "source": [
    "from askwiki import ask_wiki\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"askwiki.jsonl\",\n",
    "    target=ask_wiki,\n",
    "    evaluators={\n",
    "        \"relevance\": relevance_evaluator\n",
    "    },\n",
    "    evaluation_name=\"07-using-app-target\",\n",
    "    evaluator_config={\n",
    "        \"default\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"ground_truth\": \"${data.ground_truth}\",\n",
    "                \"response\": \"${data.response}\"\n",
    "            } \n",
    "        }\n",
    "    }\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa59de3a",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
