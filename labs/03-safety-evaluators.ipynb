{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "259d41e5",
   "metadata": {},
   "source": [
    "# Lab 03: Explore Built-in Safety Evaluators\n",
    "\n",
    "By the end of this lab, you will know:\n",
    "\n",
    "1. The built-in safety evaluators available in Azure AI Foundry\n",
    "1. How to run a safety evaluator with a test prompt (to understand usage)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6943c94",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Generation Safety Evalution\n",
    "\n",
    "The risk and safety evaluators draw on insights gained from our previous Large Language Model projects such as GitHub Copilot and Bing. This ensures a comprehensive approach to evaluating generated responses for risk and safety severity scores. These evaluators are generated through our safety evaluation service, which employs a set of LLMs. Each model is tasked with assessing specific risks that could be present in the response (for example, sexual content, violent content, etc.). These models are provided with risk definitions and severity scales, and they annotate generated conversations accordingly. \n",
    "\n",
    "Currently, we calculate a ‚Äúdefect rate‚Äù for the risk and safety evaluators below. For each of these evaluators, the service measures whether these types of content were detected and at what severity level. Each of the four types has four severity levels (Very low, Low, Medium, High). Users specify a threshold of tolerance, and the defect rates are produced by our service correspond to the number of instances that were generated at and above each threshold level. The figure below shows how the Azure AI Foundry automated safety evaluations work. [Continue reading the documentation for more details](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/evaluation-metrics-built-in?tabs=warning#risk-and-safety-evaluators)\n",
    "\n",
    "![Quality](./00-assets/automated-safety-evaluation-steps.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ea46a5d",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Understand Build-in Safety Evaluators\n",
    "\n",
    "The Azure AI Foundry plaform provides a [comprehensive set of built-in risk and safety evaluators](https://learn.microsoft.com/en-us/python/api/azure-ai-evaluation/azure.ai.evaluation.contentsafetyevaluator?view=azure-python-preview). We'll cover a small subset of them in this lab and encourage you to use the notebook environment to add your own examples to explore the rest.\n",
    "\n",
    "![Quality Evaluators](./00-assets/risk-safety-evaluators.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "925a8d9b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## üéâ | Congratulations!\n",
    "\n",
    "You have successfully completed the third lab in this module and got hands-on experience with a core subset of the the built-in safety evaluators. "
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}