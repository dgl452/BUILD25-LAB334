{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ece4e782",
   "metadata": {},
   "source": [
    "# Azure AI Evaluation client library for Python\n",
    "\n",
    "Use Azure AI Evaluation SDK to assess the performance of your generative AI applications. Generative AI application generations are quantitatively measured with mathematical based metrics, AI-assisted quality and safety metrics. Metrics are defined as evaluators. Built-in or custom evaluators can provide comprehensive insights into the application's capabilities and limitations.\n",
    "\n",
    "Use [Azure AI Evaluation SDK](https://pypi.org/project/azure-ai-evaluation/) to:\n",
    "\n",
    "- Evaluate existing data from generative AI applications\n",
    "- Evaluate generative AI applications\n",
    "- Evaluate by generating mathematical, AI-assisted quality and safety metrics\n",
    "\n",
    "Azure AI SDK provides following to evaluate Generative AI Applications:\n",
    "\n",
    "- Evaluators - Generate scores individually or when used together with evaluate API.\n",
    "- Evaluate API - Python API to evaluate dataset or application using built-in or custom evaluators."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "508e022c",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mRunning cells with 'Python 3.11.2' requires the ipykernel package.\n",
      "\u001b[1;31m<a href='command:jupyter.createPythonEnvAndSelectController'>Create a Python Environment</a> with the required packages."
     ]
    }
   ],
   "source": [
    "!pip install azure-ai-evaluation --quiet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fa127b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-identity --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3caeac9",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 1. Built-in Evaluators\n",
    "\n",
    "Built-in evaluators are out of box evaluators provided by Microsoft:\n",
    "\n",
    "| Category |\tEvaluator class |\n",
    "|:---|:--|\n",
    "| Performance and quality (AI-assisted) |\tGroundednessEvaluator, RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator, RetrievalEvaluator |\n",
    "| Performance and quality (NLP)| \tF1ScoreEvaluator, RougeScoreEvaluator, GleuScoreEvaluator, BleuScoreEvaluator, MeteorScoreEvaluator| \n",
    "| Risk and safety (AI-assisted)\t| ViolenceEvaluator, SexualEvaluator, SelfHarmEvaluator, HateUnfairnessEvaluator, IndirectAttackEvaluator, ProtectedMaterialEvaluator| \n",
    "| Composite\t| QAEvaluator, ContentSafetyEvaluator| \n",
    "\n",
    "For more in-depth information on each evaluator definition and how it's calculated, [see Evaluation and monitoring metrics for generative AI.](https://learn.microsoft.com/azure/ai-studio/concepts/evaluation-metrics-built-in)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3134076",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate a default credential\n",
    "\n",
    "from azure.identity import DefaultAzureCredential\n",
    "credential=DefaultAzureCredential()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d233f643",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Project Connection String\n",
    "connection_string = os.environ.get(\"AZURE_AI_CONNECTION_STRING\")\n",
    "\n",
    "# Extract details\n",
    "region_id, subscription_id, resource_group_name, project_name = connection_string.split(\";\")\n",
    "\n",
    "# Populate it\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group_name\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "print(azure_ai_project)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f354ed71",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import BleuScoreEvaluator\n",
    "\n",
    "# NLP bleu score evaluator\n",
    "bleu_score_evaluator = BleuScoreEvaluator()\n",
    "result = bleu_score_evaluator(\n",
    "    response=\"Tokyo is the capital of Japan.\",\n",
    "    ground_truth=\"The capital of Japan is Tokyo.\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50d2f9e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from azure.ai.evaluation import RelevanceEvaluator\n",
    "\n",
    "# AI assisted quality evaluator\n",
    "model_config = {\n",
    "    \"azure_endpoint\": os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    \"api_key\": os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    \"azure_deployment\": os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "}\n",
    "\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "result = relevance_evaluator(\n",
    "    query=\"What is the capital of Japan?\",\n",
    "    response=\"The capital of Japan is Tokyo.\"\n",
    ")\n",
    "print(result)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43c2dd08",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import ViolenceEvaluator\n",
    "\n",
    "# AI assisted safety evaluator\n",
    "violence_evaluator = ViolenceEvaluator(azure_ai_project=azure_ai_project,credential=credential)\n",
    "result = violence_evaluator(\n",
    "    query=\"What is the capital of France?\",\n",
    "    response=\"Paris.\"\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5740c62c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 2. Custom Evaluators\n",
    "Built-in evaluators are great out of the box to start evaluating your application's generations. However you can build your own code-based or prompt-based evaluator to cater to your specific evaluation needs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b27a4b4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Custom evaluator as a function to calculate response length\n",
    "def response_length(response, **kwargs):\n",
    "    return len(response)\n",
    "\n",
    "# Custom class based evaluator to check for blocked words\n",
    "class BlocklistEvaluator:\n",
    "    def __init__(self, blocklist):\n",
    "        self._blocklist = blocklist\n",
    "\n",
    "    def __call__(self, *, answer: str, **kwargs):\n",
    "        contains_block_word = any(word in answer for word in self._blocklist)\n",
    "        return {\"score\": contains_block_word}\n",
    "\n",
    "blocklist_evaluator = BlocklistEvaluator(blocklist=[\"bad\", \"worst\", \"terrible\"])\n",
    "\n",
    "# Test custom evaluator 1\n",
    "result = response_length(\"The capital of Japan is Tokyo.\")\n",
    "print(result)\n",
    "\n",
    "# Test custom evaluator 2\n",
    "result = blocklist_evaluator(answer=\"The capital of Japan is Tokyo.\")\n",
    "print(result)\n",
    "\n",
    "# Test custom evaluator 3\n",
    "result = blocklist_evaluator(answer=\"This is a bad idea.\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7862dcf7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 3. Evaluate API\n",
    "\n",
    "The package provides an evaluate API which can be used to run multiple evaluators together to evaluate generative AI application response.\n",
    "\n",
    "Let's start by evaluating responses for a test dataset\n",
    " - See: [Evaluate on test dataset using evaluate()](https://learn.microsoft.com/azure/ai-studio/how-to/develop/evaluate-sdk#evaluate-on-test-dataset-using-evaluate) for more details\n",
    "\n",
    "You can also explore evaluations directly on a target:\n",
    "- See: [Evaluate on a target](https://learn.microsoft.com/azure/ai-studio/how-to/develop/evaluate-sdk#evaluate-on-a-target)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbd4a84a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import RelevanceEvaluator, CoherenceEvaluator, FluencyEvaluator, SimilarityEvaluator, GroundednessEvaluator\n",
    "\n",
    "# provide your data here\n",
    "data=\"data.jsonl\",\n",
    "\n",
    "# configure your quality evaluators here\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "\n",
    "result = evaluate(\n",
    "    data=\"data.jsonl\", # provide your data here\n",
    "    evaluators={\n",
    "        #\"blocklist\": blocklist_evaluator,\n",
    "        \"relevance\": relevance_evaluator\n",
    "    },\n",
    "    # column mapping\n",
    "    evaluator_config={\n",
    "        \"relevance\": {\n",
    "            \"column_mapping\": {\n",
    "                \"query\": \"${data.query}\",\n",
    "                \"ground_truth\": \"${data.truth}\",\n",
    "                \"response\": \"${data.answer}\"\n",
    "            } \n",
    "        }\n",
    "    },\n",
    "    # Optionally provide your AI Foundry project information to track your evaluation results in your Azure AI Foundry project\n",
    "    azure_ai_project = azure_ai_project,\n",
    "    # Optionally provide an output path to dump a json of metric summary, row level data and metric and AI Foundry URL\n",
    "    output_path=\"./evaluation_results.json\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1872a149",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## 4. Simulator\n",
    "\n",
    "Simulators allow users to generate synthentic data using their application. Simulator expects the user to have a callback method that invokes their AI application. The intergration between your AI application and the simulator happens at the callback method. \n",
    "\n",
    "**We'll explore this in a separate lab**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
