{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# [Base Model Selection](https://learn.microsoft.com/azure/ai-foundry/concepts/evaluation-approach-gen-ai#base-model-selection)\n",
    "\n",
    "The first stage of the AI lifecycle involves selecting an appropriate base model. Generative AI models vary widely in terms of capabilities, strengths, and limitations, so it's essential to identify which model best suits your specific use case. During base model evaluation, you \"shop around\" to compare different models by testing their outputs against a set of criteria relevant to your application.\n",
    "\n",
    "You have three options to evaluate models:\n",
    "\n",
    "1. [Use Azure AI Foundry Benchmarks](https://learn.microsoft.com/en-us/azure/ai-foundry/concepts/model-benchmarks) to compare models on their intrinsic capabilities.\n",
    "1. [Use Manual Evaluations in the Portal](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/evaluate-prompts-playground) to run prompts on models and rate them.\n",
    "1. [Evaluate Multiple Models using the SDK](https://learn.microsoft.com/en-us/azure/ai-foundry/how-to/evaluate-prompts-playground) code-first\n",
    "\n",
    "**In this notebook, we explore a simplified version of option 3**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Objective\n",
    "\n",
    "You are beginning your AI application development journey - and you have two (or more) model options available to you. How do you pick the right one for your needs? In this tutorial we look at how you can evaluate _the same set of prompts_ against multiple model endpoints deployed in your Azure AI project.\n",
    "\n",
    "This guide uses Python Class as an application target which is passed to Evaluate API provided by PromptFlow SDK to evaluate results generated by LLM models against provided prompts. \n",
    "\n",
    "This tutorial uses the following Azure AI services:\n",
    "\n",
    "- [azure-ai-evaluation](https://learn.microsoft.com/en-us/azure/ai-studio/how-to/develop/evaluate-sdk)\n",
    "\n",
    "## Time\n",
    "\n",
    "You should expect to spend 30 minutes running this sample. \n",
    "\n",
    "## About this example\n",
    "\n",
    "This example demonstrates evaluating model endpoints responses against provided prompts using azure-ai-evaluation\n",
    "\n",
    "## Before you begin\n",
    "\n",
    "### Installation\n",
    "\n",
    "Install the following packages required to execute this notebook. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install azure-ai-evaluation --quiet"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Parameters and imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pprint import pprint\n",
    "\n",
    "import pandas as pd\n",
    "import random"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Target Application\n",
    "\n",
    "We will use Evaluate API provided by Prompt Flow SDK. It requires a target Application or python Function, which handles a call to LLMs and retrieve responses. \n",
    "\n",
    "In the notebook, we will use an Application Target `ModelEndpoints` to get answers from multiple model endpoints against provided question aka prompts. \n",
    "\n",
    "This application target requires list of model endpoints and their authentication keys. For simplicity, we have provided them in the `env_var` variable which is passed into init() function of `ModelEndpoints`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "endpoint = os.environ.get(\"AZURE_OPENAI_ENDPOINT\")\n",
    "api_version = os.environ.get(\"AZURE_OPENAI_API_VERSION\")\n",
    "key = os.environ.get(\"AZURE_OPENAI_KEY\")\n",
    "\n",
    "env_var = {\n",
    "    \"gpt4\": {\n",
    "        \"endpoint\": os.environ.get(\"AZURE_OPENAI_GPT4_EP\"),\n",
    "        \"key\": key,\n",
    "    },\n",
    "    \"gpt-4o-mini\": {\n",
    "        \"endpoint\": os.environ.get(\"AZURE_OPENAI_GPT4OMINI_EP\"),\n",
    "        \"key\": key,\n",
    "    },\n",
    "}\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Please provide Azure AI Project details so that traces and eval results are pushing in the project in Azure AI Studio."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'subscription_id': '6415ebd4-1dd7-430f-bd4d-2f5e9419c1cd', 'resource_group_name': 'rg-nitya-lab334', 'project_name': 'ai-project-sn5orpaatr2i6'}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Project Connection String\n",
    "connection_string = os.environ.get(\"AZURE_AI_CONNECTION_STRING\")\n",
    "\n",
    "# Extract details\n",
    "region_id, subscription_id, resource_group_name, project_name = connection_string.split(\";\")\n",
    "\n",
    "# Populate it\n",
    "azure_ai_project = {\n",
    "    \"subscription_id\": subscription_id,\n",
    "    \"resource_group_name\": resource_group_name,\n",
    "    \"project_name\": project_name,\n",
    "}\n",
    "print(azure_ai_project)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Endpoints\n",
    "The following code demonstrates how to call various model endpoints, and is configured based on `env_var` set above. For any model in `env_var`, if you do not have that model deployed in your AI project, please comment it out. If you have a model that you would like to test that does not correspond with one of the types seen below, please include that type in the `__call__` function and create a helper function to call the model's endpoint via REST. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pygmentize model_endpoints.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data\n",
    "\n",
    "Following code reads Json file \"data.jsonl\" which contains inputs to the Application Target function. It provides question, context and ground truth on each line. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                               query  \\\n",
      "0                     When was United Stated found ?   \n",
      "1                     What is the capital of France?   \n",
      "2                 Which tent is the most waterproof?   \n",
      "3         Which camping table holds the most weight?   \n",
      "4  What is the weight of the Adventure Dining Table?   \n",
      "\n",
      "                                               truth  \\\n",
      "0                                               1776   \n",
      "1                                              Paris   \n",
      "2  The Alpine Explorer Tent has the highest rainf...   \n",
      "3  The Adventure Dining Table has a higher weight...   \n",
      "4           The Adventure Dining Table weighs 15 lbs   \n",
      "\n",
      "                                              answer  \n",
      "0                                               1600  \n",
      "1                                              Paris  \n",
      "2  Can you clarify what tents you are talking about?  \n",
      "3                             Adventure Dining Table  \n",
      "4                          It's a lot I can tell you  \n"
     ]
    }
   ],
   "source": [
    "df = pd.read_json(\"data.jsonl\", lines=True)\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration\n",
    "To use Relevance and Cohenrence Evaluator, we will Azure Open AI model details as a Judge that can be passed as model config."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'azure_endpoint': 'https://aoai-sn5orpaatr2i6.openai.azure.com/', 'azure_deployment': 'gpt-4o-mini', 'api_key': '354f88b0334343e481afc7a6d5abe1d0', 'api_version': '2025-01-01-preview'}\n"
     ]
    }
   ],
   "source": [
    "from azure.ai.evaluation import AzureOpenAIModelConfiguration\n",
    "\n",
    "# Note: We are evaluating 2 models above - and we need a \"LLM Judge\" to evaluate them\n",
    "#       Here we specify the judge model\n",
    "model_config = AzureOpenAIModelConfiguration(\n",
    "    azure_endpoint=os.environ.get(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    azure_deployment=os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"),\n",
    "    api_key=os.environ.get(\"AZURE_OPENAI_API_KEY\"),\n",
    "    api_version=os.environ.get(\"AZURE_OPENAI_API_VERSION\"),\n",
    ")\n",
    "print(model_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run the evaluation\n",
    "\n",
    "The Following code runs Evaluate API and uses Content Safety, Relevance and Coherence Evaluator to evaluate results from different models.\n",
    "\n",
    "The following are the few parameters required by Evaluate API. \n",
    "\n",
    "+   Data file (Prompts): It represents data file 'data.jsonl' in JSON format. Each line contains question, context and ground truth for evaluators.     \n",
    "\n",
    "+   Application Target: It is name of python class which can route the calls to specific model endpoints using model name in conditional logic.  \n",
    "\n",
    "+   Model Name: It is an identifier of model so that custom code in the App Target class can identify the model type and call respective LLM model using endpoint URL and auth key.  \n",
    "\n",
    "+   Evaluators: List of evaluators is provided, to evaluate given prompts (questions) as input and output (answers) from LLM models. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/workspaces/contoso-chat/docs/build25-lab334/notebooks/01-select-model/data.jsonl\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-08 17:24:51 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_TARGET_20250508_172450_934483, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_TARGET_20250508_172450_934483/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-08 17:24:54 +0000  112676 execution.bulk     INFO     Process 112707 terminated.\n",
      "2025-05-08 17:24:54 +0000  112676 execution.bulk     WARNING  Process 112722 had been terminated.\n",
      "2025-05-08 17:24:54 +0000  112676 execution.bulk     WARNING  Process 112716 had been terminated.\n",
      "2025-05-08 17:24:54 +0000  112676 execution.bulk     WARNING  Process 112734 had been terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-08 17:24:55 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-05-08 17:24:55 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_relevance_20250508_172455_201279, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_relevance_20250508_172455_201279/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-08 17:24:51 +0000  111621 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-05-08 17:24:51 +0000  111621 execution          WARNING  Starting run without column mapping may lead to unexpected results. Please consult the following documentation for more information: https://aka.ms/pf/column-mapping\n",
      "2025-05-08 17:24:51 +0000  111621 execution.bulk     INFO     Set process count to 4 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 5}.\n",
      "2025-05-08 17:24:53 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-8:4)-Process id(112734)-Line number(1) start execution.\n",
      "2025-05-08 17:24:53 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-8:1)-Process id(112707)-Line number(0) start execution.\n",
      "2025-05-08 17:24:53 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-8:3)-Process id(112722)-Line number(3) start execution.\n",
      "2025-05-08 17:24:53 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-8:2)-Process id(112716)-Line number(2) start execution.\n",
      "2025-05-08 17:24:53 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-8:4)-Process id(112734)-Line number(1) completed.\n",
      "2025-05-08 17:24:53 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-8:1)-Process id(112707)-Line number(0) completed.\n",
      "2025-05-08 17:24:53 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-8:3)-Process id(112722)-Line number(3) completed.\n",
      "2025-05-08 17:24:53 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-8:4)-Process id(112734)-Line number(4) start execution.\n",
      "2025-05-08 17:24:53 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-8:2)-Process id(112716)-Line number(2) completed.\n",
      "2025-05-08 17:24:53 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-8:4)-Process id(112734)-Line number(4) completed.\n",
      "2025-05-08 17:24:54 +0000  111621 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-08 17:24:54 +0000  111621 execution.bulk     INFO     Average execution time for completed lines: 0.6 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-05-08 17:24:54 +0000  111621 execution.bulk     INFO     The thread monitoring the process [112707-ForkProcess-8:1] will be terminated.\n",
      "2025-05-08 17:24:54 +0000  111621 execution.bulk     INFO     The thread monitoring the process [112722-ForkProcess-8:3] will be terminated.\n",
      "2025-05-08 17:24:54 +0000  111621 execution.bulk     INFO     The thread monitoring the process [112716-ForkProcess-8:2] will be terminated.\n",
      "2025-05-08 17:24:54 +0000  111621 execution.bulk     INFO     The thread monitoring the process [112734-ForkProcess-8:4] will be terminated.\n",
      "2025-05-08 17:24:54 +0000  112707 execution.bulk     INFO     The process [112707] has received a terminate signal.\n",
      "2025-05-08 17:24:54 +0000  112722 execution.bulk     INFO     The process [112722] has received a terminate signal.\n",
      "2025-05-08 17:24:54 +0000  112716 execution.bulk     INFO     The process [112716] has received a terminate signal.\n",
      "2025-05-08 17:24:54 +0000  112734 execution.bulk     INFO     The process [112734] has received a terminate signal.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_TARGET_20250508_172450_934483\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-05-08 17:24:50.938218+00:00\"\n",
      "Duration: \"0:00:04.161553\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_TARGET_20250508_172450_934483\"\n",
      "\n",
      "2025-05-08 17:24:55 +0000  111621 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-05-08 17:24:56 +0000  111621 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-05-08 17:24:56 +0000  111621 execution.bulk     INFO     Average execution time for completed lines: 1.57 seconds. Estimated time for incomplete lines: 6.28 seconds.\n",
      "2025-05-08 17:24:56 +0000  111621 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-05-08 17:24:56 +0000  111621 execution.bulk     INFO     Average execution time for completed lines: 0.82 seconds. Estimated time for incomplete lines: 2.46 seconds.\n",
      "2025-05-08 17:24:57 +0000  111621 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-05-08 17:24:57 +0000  111621 execution.bulk     INFO     Average execution time for completed lines: 0.6 seconds. Estimated time for incomplete lines: 1.2 seconds.\n",
      "2025-05-08 17:24:58 +0000  111621 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-05-08 17:24:58 +0000  111621 execution.bulk     INFO     Average execution time for completed lines: 0.7 seconds. Estimated time for incomplete lines: 0.7 seconds.\n",
      "2025-05-08 17:24:58 +0000  111621 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-08 17:24:58 +0000  111621 execution.bulk     INFO     Average execution time for completed lines: 0.72 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_relevance_20250508_172455_201279\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-05-08 17:24:55.207908+00:00\"\n",
      "Duration: \"0:00:04.169311\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_relevance_20250508_172455_201279\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"relevance\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:04.169311\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_relevance_20250508_172455_201279\"\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-08 17:25:04 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_TARGET_20250508_172504_837514, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_TARGET_20250508_172504_837514/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-08 17:25:08 +0000  112968 execution.bulk     INFO     Process 113010 terminated.\n",
      "2025-05-08 17:25:08 +0000  112968 execution.bulk     WARNING  Process 113007 had been terminated.\n",
      "2025-05-08 17:25:08 +0000  112968 execution.bulk     WARNING  Process 113014 had been terminated.\n",
      "2025-05-08 17:25:08 +0000  112968 execution.bulk     WARNING  Process 112995 had been terminated.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[2025-05-08 17:25:09 +0000][promptflow._core.entry_meta_generator][WARNING] - Generate meta in current process and timeout won't take effect. Please handle timeout manually outside current process.\n",
      "[2025-05-08 17:25:09 +0000][promptflow._sdk._orchestrator.run_submitter][INFO] - Submitting run azure_ai_evaluation_evaluators_relevance_20250508_172509_055396, log path: /home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_relevance_20250508_172509_055396/logs.txt\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2025-05-08 17:25:04 +0000  111621 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-05-08 17:25:04 +0000  111621 execution          WARNING  Starting run without column mapping may lead to unexpected results. Please consult the following documentation for more information: https://aka.ms/pf/column-mapping\n",
      "2025-05-08 17:25:04 +0000  111621 execution.bulk     INFO     Set process count to 4 by taking the minimum value among the factors of {'default_worker_count': 4, 'row_count': 5}.\n",
      "2025-05-08 17:25:06 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-12:2)-Process id(113007)-Line number(1) start execution.\n",
      "2025-05-08 17:25:06 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-12:1)-Process id(112995)-Line number(0) start execution.\n",
      "2025-05-08 17:25:06 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-12:3)-Process id(113010)-Line number(2) start execution.\n",
      "2025-05-08 17:25:06 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-12:4)-Process id(113014)-Line number(3) start execution.\n",
      "2025-05-08 17:25:06 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-12:2)-Process id(113007)-Line number(1) completed.\n",
      "2025-05-08 17:25:06 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-12:1)-Process id(112995)-Line number(0) completed.\n",
      "2025-05-08 17:25:06 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-12:1)-Process id(112995)-Line number(4) start execution.\n",
      "2025-05-08 17:25:06 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-12:3)-Process id(113010)-Line number(2) completed.\n",
      "2025-05-08 17:25:06 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-12:1)-Process id(112995)-Line number(4) completed.\n",
      "2025-05-08 17:25:06 +0000  111621 execution.bulk     INFO     Process name(ForkProcess-12:4)-Process id(113014)-Line number(3) completed.\n",
      "2025-05-08 17:25:07 +0000  111621 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-08 17:25:07 +0000  111621 execution.bulk     INFO     Average execution time for completed lines: 0.6 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "2025-05-08 17:25:07 +0000  111621 execution.bulk     INFO     The thread monitoring the process [113007-ForkProcess-12:2] will be terminated.\n",
      "2025-05-08 17:25:07 +0000  111621 execution.bulk     INFO     The thread monitoring the process [113010-ForkProcess-12:3] will be terminated.\n",
      "2025-05-08 17:25:07 +0000  111621 execution.bulk     INFO     The thread monitoring the process [112995-ForkProcess-12:1] will be terminated.\n",
      "2025-05-08 17:25:07 +0000  113007 execution.bulk     INFO     The process [113007] has received a terminate signal.\n",
      "2025-05-08 17:25:07 +0000  111621 execution.bulk     INFO     The thread monitoring the process [113014-ForkProcess-12:4] will be terminated.\n",
      "2025-05-08 17:25:07 +0000  113010 execution.bulk     INFO     The process [113010] has received a terminate signal.\n",
      "2025-05-08 17:25:07 +0000  112995 execution.bulk     INFO     The process [112995] has received a terminate signal.\n",
      "2025-05-08 17:25:07 +0000  113014 execution.bulk     INFO     The process [113014] has received a terminate signal.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_TARGET_20250508_172504_837514\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-05-08 17:25:04.840091+00:00\"\n",
      "Duration: \"0:00:04.136659\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_TARGET_20250508_172504_837514\"\n",
      "\n",
      "2025-05-08 17:25:09 +0000  111621 execution.bulk     INFO     Current thread is not main thread, skip signal handler registration in BatchEngine.\n",
      "2025-05-08 17:25:11 +0000  111621 execution.bulk     INFO     Finished 1 / 5 lines.\n",
      "2025-05-08 17:25:11 +0000  111621 execution.bulk     INFO     Average execution time for completed lines: 2.23 seconds. Estimated time for incomplete lines: 8.92 seconds.\n",
      "2025-05-08 17:25:11 +0000  111621 execution.bulk     INFO     Finished 2 / 5 lines.\n",
      "2025-05-08 17:25:11 +0000  111621 execution.bulk     INFO     Average execution time for completed lines: 1.18 seconds. Estimated time for incomplete lines: 3.54 seconds.\n",
      "2025-05-08 17:25:11 +0000  111621 execution.bulk     INFO     Finished 3 / 5 lines.\n",
      "2025-05-08 17:25:11 +0000  111621 execution.bulk     INFO     Average execution time for completed lines: 0.8 seconds. Estimated time for incomplete lines: 1.6 seconds.\n",
      "2025-05-08 17:25:12 +0000  111621 execution.bulk     INFO     Finished 4 / 5 lines.\n",
      "2025-05-08 17:25:12 +0000  111621 execution.bulk     INFO     Average execution time for completed lines: 0.73 seconds. Estimated time for incomplete lines: 0.73 seconds.\n",
      "2025-05-08 17:25:14 +0000  111621 execution.bulk     INFO     Finished 5 / 5 lines.\n",
      "2025-05-08 17:25:14 +0000  111621 execution.bulk     INFO     Average execution time for completed lines: 0.99 seconds. Estimated time for incomplete lines: 0.0 seconds.\n",
      "======= Run Summary =======\n",
      "\n",
      "Run name: \"azure_ai_evaluation_evaluators_relevance_20250508_172509_055396\"\n",
      "Run status: \"Completed\"\n",
      "Start time: \"2025-05-08 17:25:09.061340+00:00\"\n",
      "Duration: \"0:00:05.070868\"\n",
      "Output path: \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_relevance_20250508_172509_055396\"\n",
      "\n",
      "======= Combined Run Summary (Per Evaluator) =======\n",
      "\n",
      "{\n",
      "    \"relevance\": {\n",
      "        \"status\": \"Completed\",\n",
      "        \"duration\": \"0:00:05.070868\",\n",
      "        \"completed_lines\": 5,\n",
      "        \"failed_lines\": 0,\n",
      "        \"log_path\": \"/home/vscode/.promptflow/.runs/azure_ai_evaluation_evaluators_relevance_20250508_172509_055396\"\n",
      "    }\n",
      "}\n",
      "\n",
      "====================================================\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import pathlib\n",
    "\n",
    "from azure.ai.evaluation import evaluate\n",
    "from azure.ai.evaluation import (\n",
    "    RelevanceEvaluator,\n",
    ")\n",
    "from model_endpoints import ModelEndpoints\n",
    "\n",
    "relevance_evaluator = RelevanceEvaluator(model_config)\n",
    "\n",
    "models = [\n",
    "    \"gpt4\",\n",
    "    \"gpt-4o-mini\",\n",
    "]\n",
    "\n",
    "path = str(pathlib.Path(pathlib.Path.cwd())) + \"/data.jsonl\"\n",
    "print(path)\n",
    "\n",
    "for model in models:\n",
    "    randomNum = random.randint(1111, 9999)\n",
    "    results = evaluate(\n",
    "        evaluation_name=\"Eval-Run-\" + str(randomNum) + \"-\" + model.title(),\n",
    "        data=path,\n",
    "        target=ModelEndpoints(env_var, model),\n",
    "        evaluators={\n",
    "            \"relevance\": relevance_evaluator,\n",
    "        },\n",
    "        azure_ai_project=azure_ai_project,\n",
    "        evaluator_config={\n",
    "            \"relevance\": {\n",
    "                \"column_mapping\": {\n",
    "                    \"response\": \"${target.response}\",\n",
    "                    \"context\": \"${data.truth}\",\n",
    "                    \"query\": \"${data.query}\",\n",
    "                },\n",
    "            },\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "View the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(results[\"rows\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
